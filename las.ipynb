{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "trainer_las.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inokchoi/Speech/blob/main/las.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jWLYz4B6l02"
      },
      "source": [
        "리스너튼 인코더, 스펠러는 디코더이다.\n",
        "피라미드 아키텍쳐로 바이 디렉션 \n",
        "임베딩 에이치\n",
        "스펠러에 에이치 집어넣으면\n",
        "케릭터로 나온다.\n",
        "\n",
        "음성 루틴은 거의 같기 때문에 모델만 바꾸면 된다.\n",
        "컨텍스트 찾아가는 부분 (C_i, S_i)\n",
        "\n",
        "라스 티쳐프로세싱\n",
        "stpo criterien이 구현이 안되어있다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtjOjDmmsnzK",
        "outputId": "be4f85d2-0723-4a90-e63e-579ca2219cf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3wH4CZJso9q"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8FcsxFbsoqD",
        "outputId": "ca40c43d-b624-473c-92db-c8cd8ce5f725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd '/content/drive/My Drive/Colab Notebooks/20-1voice_interface/Proj3/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/20-1voice_interface/Proj3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prt5ae4t7XNI",
        "outputId": "33e56d80-cf24-4e97-e432-a9ce465eba10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "!pip install editdistance\n",
        "!pip install logger\n",
        "!pip install tensorboard_logger\n",
        "!pip install easydict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (0.5.3)\n",
            "Collecting logger\n",
            "  Downloading https://files.pythonhosted.org/packages/73/2f/b0d28eaa1e2c1cf64129f8da3fe701888d152677fec708cd0f13e8309e1e/logger-1.4.tar.gz\n",
            "Building wheels for collected packages: logger\n",
            "  Building wheel for logger (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for logger: filename=logger-1.4-cp36-none-any.whl size=1789 sha256=22c413c0f3050f489912b8c92537ce80e08b6fa866abe84f5e119d032f890cba\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/d4/96/08341e2ac92c1ed4b760e4848e1acda3795f0257a83b94b42e\n",
            "Successfully built logger\n",
            "Installing collected packages: logger\n",
            "Successfully installed logger-1.4\n",
            "Collecting tensorboard_logger\n",
            "  Downloading https://files.pythonhosted.org/packages/87/7a/ec0fd26dba69191f82eb8f38f5b401c124f45a207490a7ade6ea9717ecdb/tensorboard_logger-0.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboard_logger) (1.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboard_logger) (1.12.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from tensorboard_logger) (3.10.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard_logger) (1.4.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard_logger) (7.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->tensorboard_logger) (46.3.0)\n",
            "Installing collected packages: tensorboard-logger\n",
            "Successfully installed tensorboard-logger-0.1.0\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.6/dist-packages (1.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDb6-W5dsmmm"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pdb\n",
        "import numpy as np\n",
        "import sys\n",
        "import preprocess\n",
        "import pdb\n",
        "import os\n",
        "import editdistance\n",
        "import time\n",
        "import easydict \n",
        "from torch.utils.data import DataLoader\n",
        "from dataset import *\n",
        "from model import *\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "from logger import Logger\n",
        "\n",
        "# Store log information in the designated directory.\n",
        "logger = Logger('./las_log/{}'.format(time.ctime()).replace(' ','_').replace(':','_'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwOh6xXgsmmq"
      },
      "source": [
        "## Decode and LER calculation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvdEZoJdsmmr"
      },
      "source": [
        "def decode(pred, label, target_len, phn_list):\n",
        "    ler = 0\n",
        "    for i in range(pred.size(0)):\n",
        "        # Find the maximum likelihood output.\n",
        "        pred_class = torch.argmax(pred[i,:,:], dim=-1)\n",
        "        \n",
        "        # Collapse the repeated outputs. (Note: simplified implementation)\n",
        "        collapse = [pred_class[k] for k in range(len(pred_class)) if (k==0) or pred_class[k] != pred_class[k-1]]\n",
        "        \n",
        "        # Compute the edit distance between the reference and estimated ones.\n",
        "        collapse = torch.tensor([value for value in collapse if value != 0])        \n",
        "        ed = editdistance.eval(collapse, label[i][:target_len[i]])/target_len[i]        \n",
        "        ler += ed\n",
        "        \n",
        "    ler /= pred.size(0)\n",
        "    \n",
        "    return ler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPPxu0Zfsmmt"
      },
      "source": [
        "## Inference function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmeXVWIFsmmu"
      },
      "source": [
        "def inference(epoch, model, data_loader, criterion, optimizer, device, \n",
        "              phn_list, train=True):\n",
        "    if train:\n",
        "        model.train()\n",
        "        flag = 'Train'\n",
        "    else:\n",
        "        model.eval()\n",
        "        flag = 'Eval'\n",
        "\n",
        "    total_loss = 0\n",
        "    total_ler = 0\n",
        "    progress_bar = tqdm(enumerate(data_loader))\n",
        "    \n",
        "    for b_idx, (data, label, input_lengs, target_lengs) in progress_bar:\n",
        "        # Move data and label to the device (e.g. GPU).\n",
        "        data = data.type(torch.FloatTensor).to(device)\n",
        "        label = label.type(torch.LongTensor).to(device)\n",
        "        \n",
        "        # Predict the output given the input data.\n",
        "        if train:\n",
        "            pred, att_score, out_label = model(data, label, 1.0, train=train)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                pred, att_score, out_label = model(data, label, 1.0, train=train)\n",
        "                \n",
        "        # Compute the loss by comparing the predicted output to the reference labels.\n",
        "        loss = criterion(torch.log(pred).transpose(1,2), out_label[:,:pred.size(1)])\n",
        "        total_loss += loss\n",
        "\n",
        "        # Decode and calculate LER.\n",
        "        ler = decode(pred, out_label, target_lengs, phn_list)\n",
        "        total_ler += ler\n",
        "        \n",
        "        # Perform backpropagation if it is the training mode.\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "          \n",
        "        # Display the running progress.\n",
        "        progress_bar.set_description(\n",
        "            '{} Epoch:{}[{}/{}] CE_loss:{:.3f} LER:{:.3f} '.format( \\\n",
        "            flag, epoch, b_idx, len(data_loader), total_loss/(b_idx+1), \n",
        "            total_ler/(b_idx+1))\n",
        "        )\n",
        "        \n",
        "    return total_loss, total_ler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqtZaGrdsmmx"
      },
      "source": [
        "## Main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAGBbVCDsmmx"
      },
      "source": [
        "def main():\n",
        "    # Set the environmental parameters.\n",
        "    args = easydict.EasyDict({\n",
        "      'train_list':'./timit_train_list.txt',\n",
        "      'test_list':'./timit_test_list.txt',\n",
        "      'data_path':'./',\n",
        "      'batch_size':4,\n",
        "      'input_type':'mfcc',\n",
        "      'input_dim':13,\n",
        "      'delta_order':3,\n",
        "      'resume':False,\n",
        "      'resume_model':'las_10.pth',\n",
        "      'lr':0.001,\n",
        "      'max_epoch':100,\n",
        "      'log_path':'./logs',\n",
        "      'save_dir':'./models',\n",
        "      \n",
        "    })\n",
        "  \n",
        "    # Set the device for running the LAS model.\n",
        "    device  = torch.device('cuda:0')\n",
        "\n",
        "    hidden_dim = 256\n",
        "    l_layer = 3\n",
        "    s_layer = 1\n",
        "    num_class = 40 + 1 + 1 +1      # 40 phoneme + <sos, eos, pad>  #pad \n",
        "    \n",
        "    # Define a network architecture. ('model.py')\n",
        "    model = LAS(args.input_dim*args.delta_order, hidden_dim, l_layer,\n",
        "                hidden_dim+num_class, hidden_dim, num_class, s_layer)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Define an optimizer.\n",
        "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    # Define a training criterion.\n",
        "    las_criterion = torch.nn.NLLLoss(ignore_index=0) # ignore <pad> labels  #C개 클래스를 클래시피케이션할 때 사용되는 것이다. negative log likelihood,\n",
        "\n",
        "    # Load the pre-trained model if you resume the training from the model.\n",
        "    os.makedirs(args.save_dir, exist_ok=True)\n",
        "    if args.resume:\n",
        "        if os.path.isfile(os.path.join(args.save_dir, args.resume_model)):\n",
        "            checkpoint = torch.load(os.path.join(args.save_dir, args.resume_model),map_location=device)\n",
        "            model.load_state_dict(checkpoint['encoder'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            start = checkpoint['epoch']\n",
        "            print('----loading checkpoint----')\n",
        "        else:\n",
        "            print('can not find the resume model.')\n",
        "    else:\n",
        "        start=0\n",
        "\n",
        "    # Set the phone list for decoding. ('preprocess.py')\n",
        "    phn_list = preprocess.phn_list()\n",
        "\n",
        "    # Define training and test datasets. ('dataset.py')\n",
        "    train_dataset = ASR_Dataset(args.train_list, args.data_path, d_type=args.input_type, coeff=args.input_dim, delta=args.delta_order)\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                               batch_size = args.batch_size,\n",
        "                               collate_fn = collate_fn,\n",
        "                               shuffle=True,\n",
        "                               num_workers=4,\n",
        "                               pin_memory=True)\n",
        "\n",
        "    test_dataset = ASR_Dataset(args.test_list, args.data_path, d_type=args.input_type, coeff=args.input_dim, delta=args.delta_order)\n",
        "    test_loader = DataLoader(test_dataset,\n",
        "                              batch_size=1,\n",
        "                              collate_fn = collate_fn,\n",
        "                              shuffle=False,\n",
        "                              num_workers=4,\n",
        "                              pin_memory=True)\n",
        "    \n",
        "    # Perform training/validation processing.\n",
        "    for epoch in range(start, args.max_epoch+1):        \n",
        "        train_loss, train_ler = inference(epoch, model, train_loader, las_criterion, \n",
        "                                          optimizer, device, phn_list, train=True)        \n",
        "        valid_loss, valid_ler = inference(epoch, model, test_loader, las_criterion, \n",
        "                                          optimizer, device, phn_list, train=False)\n",
        "        \n",
        "        # Save the trained model at every 10 epochs.\n",
        "        if epoch % 10 == 0:\n",
        "            torch.save({'epoch':epoch, 'encoder':model.state_dict(),\n",
        "                        'optimizer':optimizer.state_dict()},\n",
        "                        '{}/las_{}.pth'.format(args.save_dir, epoch))\n",
        "\n",
        "        # Log the loss and LER.\n",
        "        logger.log_value('train_loss',train_loss)\n",
        "        logger.log_value('train_ler',train_ler)\n",
        "        logger.log_value('valid_loss',valid_loss)\n",
        "        logger.log_value('valid_ler',valid_ler)\n",
        "        logger.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KEvBE50Tsmmz",
        "outputId": "0f2b88cb-aa3b-4e79-e222-5913dea96e20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "# run the main functoin\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch:0[227/228] CE_loss:3.226 LER:0.958 : : 228it [06:02,  1.59s/it]\n",
            "Eval Epoch:0[294/295] CE_loss:3.193 LER:0.832 : : 295it [02:20,  2.09it/s]\n",
            "Train Epoch:1[227/228] CE_loss:3.081 LER:0.620 : : 228it [03:00,  1.27it/s]\n",
            "Eval Epoch:1[294/295] CE_loss:3.121 LER:0.594 : : 295it [01:04,  4.59it/s]\n",
            "Train Epoch:2[227/228] CE_loss:2.958 LER:0.497 : : 228it [03:09,  1.20it/s]\n",
            "Eval Epoch:2[175/295] CE_loss:3.064 LER:0.563 : : 176it [00:38,  5.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-df38050f8fd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# run the main functoin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-5f1d75d1133c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m                                           optimizer, device, phn_list, train=True)        \n\u001b[1;32m     77\u001b[0m         valid_loss, valid_ler = inference(epoch, model, test_loader, las_criterion, \n\u001b[0;32m---> 78\u001b[0;31m                                           optimizer, device, phn_list, train=False)\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Save the trained model at every 10 epochs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9e7da5e5fb42>\u001b[0m in \u001b[0;36minference\u001b[0;34m(epoch, model, data_loader, criterion, optimizer, device, phn_list, train)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Compute the loss by comparing the predicted output to the reference labels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/20-1voice_interface/Proj3/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, ground_truth, tf_rate, train)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Predic output characters/phonemes using the high-level embeddings, h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/20-1voice_interface/Proj3/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, listener_feat, ground_truth, tf_rate, train)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# Forward each time step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlistener_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mword_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0matt_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/20-1voice_interface/Proj3/model.py\u001b[0m in \u001b[0;36mforward_step\u001b[0;34m(self, input_word, last_hidden, listener_feat)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# Compute attention and context c_i from the rnn_output s_i and listener features h.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mattention_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlistener_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Predict the word character from the rnn_out s_i and context c_i.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/20-1voice_interface/Proj3/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_state, listener_feat)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \"\"\"\n\u001b[1;32m    190\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlistener_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mattention_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlistener_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuGDvUF1smm3"
      },
      "source": [
        "librispeech 검색 : 데이터가 많이 있다.\n",
        "이걸 가져와서 쓰면 큰 음성인식 시스템을 쓸 수 있다.\n",
        "\n",
        "구글: las 많이 씀\n",
        "아마존: las, ctc\n",
        "대부분 최신 ai 스피커는 서버방식이므로 2개 중 하나 씀\n",
        "일부 옛날 히든마크포모델+딥러닝 쓰는 경우도 있을 듯\n",
        "ctc를 확장해서 쓰는 기관들이 더 많은 것 같다. (바이두, 페이스북)\n",
        "\n",
        "언어별로 ctc, las중 잘 되는게  수 있다.\n",
        "\n",
        "\n",
        "같은 모델이라도 어떤 하이퍼 파라미터를 쓰는지에달라질 따라 성능이 달라진다.\n",
        "학습은 노하우가 필요하고,\n",
        "학습이 좋으면 모델이 나빠도 성능을 높일 수 있다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6hRLNX5_k5S"
      },
      "source": [
        ""
      ]
    }
  ]
}