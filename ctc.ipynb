{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "trainer_ctc.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inokchoi/Speech/blob/main/ctc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5o6cUEiwDWZ"
      },
      "source": [
        "히든마커 + 딥러닝 (하이브리드)\n",
        "딥러닝만은 CTC, LAS\n",
        "\n",
        "TIMIT - 3개 기관이 만듦, MIT, SRI, TI\n",
        "6300개 센텐스, 사람 630명, 10개 발음\n",
        "미국 8개 사투리 모아놓음\n",
        "DR1, 2만 주셨음 (New England, Northern)\n",
        "Dialect만 쓰게 될 것입니다.\n",
        "\n",
        "TIMIT의 PHN 내용을 보면 각각의 segmentation이 되어있다. (마크가 되어있음)\n",
        "WRD는 워드로 레이블링 되어있다.\n",
        "\n",
        "RAW, WRD, PHN이 저장이 되어있어서 좋다.\n",
        "\n",
        "데이터가 Training, test가 같은 것도 있다. (저작권 때문에)\n",
        "\n",
        "phn_list.txt 보면 맨 앞이 음소(aa) 61개가 있고, 중복된 소리를 묶어서 40개정도만 쓴다. (맨 오른쪽)\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lI9nk4bzCw8"
      },
      "source": [
        "ipynb: 메인 파일\n",
        "\n",
        "py파일: 파이썬 파일\n",
        "\n",
        "phn_list.txt\n",
        "\n",
        "화자인식처럼 list.txt 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7aLkDKZsY8H",
        "outputId": "a074c986-8754-458a-b4c6-9c8378c0aaae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZBpiso1sbmV"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNwlZyoSsdr8",
        "outputId": "5deb8e7b-a708-4d83-f54c-5c74610e2500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd '/content/drive/My Drive/Colab Notebooks/20-1voice_interface/Proj3/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/20-1voice_interface/Proj3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hDju1VJsX8h",
        "outputId": "23bb209c-57db-42ac-af7c-815b9c3ae6dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "!pip install editdistance\n",
        "!pip install logger\n",
        "!pip install tensorboard_logger\n",
        "!pip install easydict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (0.5.3)\n",
            "Collecting logger\n",
            "  Downloading https://files.pythonhosted.org/packages/73/2f/b0d28eaa1e2c1cf64129f8da3fe701888d152677fec708cd0f13e8309e1e/logger-1.4.tar.gz\n",
            "Building wheels for collected packages: logger\n",
            "  Building wheel for logger (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for logger: filename=logger-1.4-cp36-none-any.whl size=1789 sha256=1d6040cfd06ef4f1fb34be3b8f4b3d712bb649daf4ba8b5979b0be8fc5d024f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/d4/96/08341e2ac92c1ed4b760e4848e1acda3795f0257a83b94b42e\n",
            "Successfully built logger\n",
            "Installing collected packages: logger\n",
            "Successfully installed logger-1.4\n",
            "Collecting tensorboard_logger\n",
            "  Downloading https://files.pythonhosted.org/packages/87/7a/ec0fd26dba69191f82eb8f38f5b401c124f45a207490a7ade6ea9717ecdb/tensorboard_logger-0.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard_logger) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboard_logger) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboard_logger) (1.18.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from tensorboard_logger) (3.10.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard_logger) (7.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->tensorboard_logger) (46.3.0)\n",
            "Installing collected packages: tensorboard-logger\n",
            "Successfully installed tensorboard-logger-0.1.0\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.6/dist-packages (1.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueQ7E6Z8sX8k"
      },
      "source": [
        "import torch\n",
        "import pdb\n",
        "import numpy as np\n",
        "import sys\n",
        "import preprocess\n",
        "import pdb\n",
        "import os\n",
        "import editdistance\n",
        "import time\n",
        "import easydict\n",
        "from torch.utils.data import DataLoader\n",
        "from dataset import *\n",
        "from model import *\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "from logger import Logger\n",
        "\n",
        "# Store log information in the designated directory.\n",
        "logger = Logger('./ctc_log/{}'.format(time.ctime()).replace(' ','_').replace(':', '_')) #로그 인포메이션 저장 (트레이닝 중간 결과 저장)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2Cr8vsYsX8p"
      },
      "source": [
        "## Decode and LER calculation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyC59I6Czu_x"
      },
      "source": [
        "LER: Label error rate 측정하는 것. 길이로 나눠서 구한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlqKpv3VsX8p"
      },
      "source": [
        "def decode(pred, label, target_len, phn_list): #prediction된 결과를 가지고 maximum 찾고, collapse.\n",
        "    ler = 0\n",
        "    for i in range(pred.size(1)):\n",
        "        # Find the maximum likelihood output.\n",
        "        pred_class = torch.argmax(pred[:,i,:], dim=-1)\n",
        "        \n",
        "        # Collapse the repeated outputs. (Note: simplified implementation)\n",
        "        collapse = [pred_class[k] for k in range(len(pred_class)) if (k==0) or pred_class[k] != pred_class[k-1]]\n",
        "        \n",
        "        # Compute the edit distance between the reference and estimated ones.\n",
        "        collapse = torch.tensor([value for value in collapse if value != 0])\n",
        "        ed = editdistance.eval(collapse, label[i][:target_len[i]])/target_len[i]  #editdistance 얼마만큼 에러 레이트인지를 볼 수 있게 한다.\n",
        "        ler += ed\n",
        "    \n",
        "    ler /= pred.size(1)\n",
        "    \n",
        "    return ler, collapse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijal4RZ1sX8s"
      },
      "source": [
        "## Inference function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SVoCSM1sX8t"
      },
      "source": [
        "def inference(epoch, model, data_loader, criterion, optimizer, device, \n",
        "              phn_list, train=True):\n",
        "    if train:\n",
        "        model.train()\n",
        "        flag = 'Train'\n",
        "    else:\n",
        "        model.eval()\n",
        "        flag='Eval'\n",
        "\n",
        "    total_loss = 0\n",
        "    total_ler = 0\n",
        "    progress_bar = tqdm(enumerate(data_loader)) #프린트가 많아지면 문장이 안보여서 추가함\n",
        "  \n",
        "    for b_idx, (data, label, input_lengs, target_lengs) in progress_bar:\n",
        "        # Move data and label to the device (e.g. GPU).\n",
        "        data = data.type(torch.FloatTensor).to(device)  \n",
        "        label = label.type(torch.LongTensor).to(device)  #Label은 integer type이어서\n",
        "        \n",
        "        # Predict the output given the input data.\n",
        "        if train:\n",
        "            pred = model(data)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                pred = model(data)\n",
        "                \n",
        "        # Compute the loss by comparing the predicted output to the reference labels.\n",
        "        loss = criterion(pred, label, input_lengs, target_lengs)  #CTCLoss\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "        # Decode and calculate LER.\n",
        "        ler, collapse = decode(pred, label, target_lengs, phn_list) #모델 만든걸 디코드 해서 에러율 알려고\n",
        "        total_ler += ler\n",
        "        \n",
        "        # Perform backpropagation if it is the training mode.\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        # Display the running progress.\n",
        "        progress_bar.set_description(\n",
        "            '{} Epoch:{}[{}/{}] CTC_loss:{:.3f} LER:{:.3f} '.format( \\\n",
        "            flag, epoch, b_idx, len(data_loader), total_loss/(b_idx+1), \n",
        "            total_ler/(b_idx+1))\n",
        "        )\n",
        "  \n",
        "    return total_loss, total_ler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH_XbBDMsX8w"
      },
      "source": [
        "## Main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTuiZrULsX8x"
      },
      "source": [
        "def main():\n",
        "    # Set the environmental parameters.\n",
        "    args = easydict.EasyDict({\n",
        "      'train_list':'./timit_train_list.txt',\n",
        "      'test_list':'./timit_test_list.txt',\n",
        "      'data_path':'./',  #실제 웨이브파일\n",
        "      'batch_size':8,\n",
        "      'input_type':'mfcc',\n",
        "      'input_dim':13,\n",
        "      'delta_order':3, #델타 쓸거냐 말꺼냐\n",
        "      'resume':False, #세이브된것부터 하고 싶을 때\n",
        "      'resume_model':'ctc_10.pth', #모델 저장 파일\n",
        "      'lr':0.01,\n",
        "      'max_epoch':20,\n",
        "      'log_path':'./logs',\n",
        "      'save_dir':'./models',       \n",
        "    })\n",
        "  \n",
        "    # Set the device for running the CTC model.\n",
        "    device  = torch.device('cuda:0')\n",
        "\n",
        "    # Define a network architecture. ('model.py')\n",
        "    model = CTC_model(input_dim=args.input_dim*args.delta_order, num_class = 41)\n",
        "    model = model.to(device) #이걸 안하면 CPU로 한다.\n",
        "\n",
        "    # Define an optimizer.\n",
        "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    # Define a training criterion.\n",
        "    ctc_criterion = torch.nn.CTCLoss(blank=0)\n",
        "\n",
        "    # Load the pre-trained model if you resume the training from the model.\n",
        "    os.makedirs(args.save_dir, exist_ok=True)\n",
        "    if args.resume:  #이미 수행된 모델이 있으면 모델 읽어서 로드하기.\n",
        "        if os.path.isfile(os.path.join(args.save_dir, args.resume_model)):\n",
        "            checkpoint = torch.load(os.path.join(args.save_dir, args.resume_model),map_location=device)\n",
        "            model.load_state_dict(checkpoint['encoder'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            start = checkpoint['epoch']\n",
        "            print('----loading checkpoint----')\n",
        "        else:\n",
        "            print('can not find the resume model.')\n",
        "    else:\n",
        "        start=0\n",
        "\n",
        "    # Set the phone list for decoding. ('preprocess.py')\n",
        "    phn_list = preprocess.phn_list() #61리스트 가져온다. 3개 열 중 맨 뒤에꺼만 가져온다. 코드 보면 리스트를 set으로 같은건 만든다.\n",
        "\n",
        "    # Define training and test datasets. ('dataset.py')\n",
        "    train_dataset = ASR_Dataset(args.train_list, args.data_path, d_type=args.input_type, #mel, mfcc쓸거냐 get feature하는 것, collat 잘 하면 문제 없을 것이다.\n",
        "                                coeff=args.input_dim, delta=args.delta_order)\n",
        "    train_loader = DataLoader(train_dataset, #화자인식 때 DataLoader 했음, 영상도 어떻게 feeding할지만 만들어놓으면 쉬움\n",
        "                              batch_size = args.batch_size,\n",
        "                              collate_fn = collate_fn,\n",
        "                              shuffle=True,\n",
        "                              num_workers=4,\n",
        "                              pin_memory=True)\n",
        "\n",
        "    test_dataset = ASR_Dataset(args.test_list, args.data_path, d_type=args.input_type,\n",
        "                               coeff=args.input_dim, delta=args.delta_order)\n",
        "    test_loader = DataLoader(test_dataset,\n",
        "                             batch_size=1,\n",
        "                             collate_fn = collate_fn,\n",
        "                             shuffle=False,\n",
        "                             num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "    # Perform training/validation processing.\n",
        "    for epoch in range(start, args.max_epoch+1):\n",
        "        train_loss, train_ler = inference(epoch, model, train_loader, ctc_criterion, \n",
        "                                          optimizer, device, phn_list, train=True)\n",
        "        valid_loss, valid_ler = inference(epoch, model, test_loader, ctc_criterion, \n",
        "                                          optimizer, device, phn_list, train=False)\n",
        "        \n",
        "        # Save the trained model at every 10 epochs.\n",
        "        if epoch % 10 == 0:\n",
        "            torch.save({'epoch':epoch, 'encoder':model.state_dict(),\n",
        "                        'optimizer':optimizer.state_dict()},\n",
        "                        '{}/ctc_{}.pth'.format(args.save_dir, epoch))\n",
        "\n",
        "        # Log the loss and LER.\n",
        "        logger.log_value('train_loss', train_loss)\n",
        "        logger.log_value('train_ler', train_ler)\n",
        "        logger.log_value('valid_loss', valid_loss)\n",
        "        logger.log_value('valid_ler', valid_ler)\n",
        "        logger.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "afDkyH6csX8z",
        "outputId": "8c5e4e44-834d-4ec2-c184-1b931c7466b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "# Run the main functoin.\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch:0[113/114] CTC_loss:3.970 LER:0.984 : : 114it [06:01,  3.17s/it]\n",
            "Eval Epoch:0[219/295] CTC_loss:3.301 LER:0.969 : : 219it [01:57,  1.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b057a4c75e04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run the main functoin.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-6e2332411198>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m                                           optimizer, device, phn_list, train=True)\n\u001b[1;32m     72\u001b[0m         valid_loss, valid_ler = inference(epoch, model, test_loader, ctc_criterion, \n\u001b[0;32m---> 73\u001b[0;31m                                           optimizer, device, phn_list, train=False)\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Save the trained model at every 10 epochs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ce46f0f93d54>\u001b[0m in \u001b[0;36minference\u001b[0;34m(epoch, model, data_loader, criterion, optimizer, device, phn_list, train)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#프린트가 많아지면 문장이 안보여서 추가함\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mb_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Move data and label to the device (e.g. GPU).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvOGq6iOsX83"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dnkYeQd4kur"
      },
      "source": [
        "LER이 1을 넘을 수도 있다.\n",
        "20프로 정도 됐을 때 트레이닝이 잘 됐다고 할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjzBvypN5l7Z"
      },
      "source": [
        "mel 줄이는게 차수를 줄이는게 더 효율적이다.\n",
        "데이터 많으면 mel filter 안쓰고 linear 써도 된다.\n",
        "적은 데이터로는 mel 쓴다.\n",
        "\n",
        "mel -- log mel filter bank의 dct를 하게 되면 그게 mfcc가 된다.\n",
        "log mel은 주파수 도메인 에너지\n",
        "mfcc는 한번 더 푸리에 해서 캡스럴 콘시트가 ?\n",
        "각각 엘리먼트가 인디펜던트하게 나오게 된다.\n",
        "데이터 적을 때 더 효과적이다.\n",
        "\n",
        "stft변환 시 2의 n승을 쓰는게 가까운게 보편적.\n",
        "n+1은 정보차는 없지만 분해능은 더 좋다."
      ]
    }
  ]
}