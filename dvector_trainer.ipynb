{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "PJ4_trainer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inokchoi/Speech/blob/main/dvector_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ402iJMpRPM"
      },
      "source": [
        "# d-vector Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywTOq6WppRPN"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHdW9ZYnqbJ3",
        "outputId": "4aee3648-8cfb-4b5d-8a73-93714fe11e57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfn-1Vljqg_v"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWArtl5RqrHX",
        "outputId": "e4ebc5a1-aeae-49fb-e8fa-42b13507a02e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd '/content/drive/My Drive/Colab Notebooks/20-1voice_interface/Proj2/dvector/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/20-1voice_interface/Proj2/dvector\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV-Csss4WYXl"
      },
      "source": [
        "#import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dITJUGcpZI0O"
      },
      "source": [
        "#train_list = glob.glob('../wsj0/*/*.wav')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0Cyjd8QZTlm"
      },
      "source": [
        "#train_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KkbCoV9W7HT",
        "outputId": "88afe070-785d-4520-d577-07f49cd67121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "#speaker_list = [train_list[i].split('/')[-1].split('\\\\')[-1].split('_')[0] for i in range(len(train_list))]\n",
        "speaker_list = [train_list[i][2:] for i in range(len(train_list))]\n",
        "s = set([])\n",
        "test_list = []\n",
        "with open('wsj_train1.txt','w') as f:\n",
        "    count = 0\n",
        "    for spk in speaker_list:\n",
        "        if count % 14 == 0:\n",
        "          s.add(spk.split('/')[2])\n",
        "          test_list.append(str(len(s)-1) + ' ' + spk+'\\n')\n",
        "        else:\n",
        "          s.add(spk.split('/')[2])\n",
        "          f.write(str(len(s)-1) + ' ' + spk+'\\n')\n",
        "        count += 1\n",
        "\n",
        "with open('wsj_test1.txt','w') as f:\n",
        "    for spk in test_list:\n",
        "        f.write(spk)\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#speaker_list = [train_list[i].split('/')[-1].split('\\\\')[-1].split('_')[0] for i in range(len(train_list))]\\nspeaker_list = [train_list[i][2:] for i in range(len(train_list))]\\ns = set([])\\ntest_list = []\\nwith open('wsj_train1.txt','w') as f:\\n    count = 0\\n    for spk in speaker_list:\\n        if count % 14 == 0:\\n          s.add(spk.split('/')[2])\\n          test_list.append(str(len(s)-1) + ' ' + spk+'\\n')\\n        else:\\n          s.add(spk.split('/')[2])\\n          f.write(str(len(s)-1) + ' ' + spk+'\\n')\\n        count += 1\\n\\nwith open('wsj_test1.txt','w') as f:\\n    for spk in test_list:\\n        f.write(spk)\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmUxrXn6aCBu"
      },
      "source": [
        "#speaker_list = [test_list[i][2:] for i in range(len(test_list))]\n",
        "#s = set([])\n",
        "#with open('wsj_test1.txt','w') as f:\n",
        "#    for spk in speaker_list:\n",
        "#        s.add(spk.split('/')[2])\n",
        "#        f.write(str(len(s)-1) + ' ' + spk+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rREadvKTpRPO"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm #프린트 예쁘게\n",
        "import numpy as np\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "from PJ4_dataset import *\n",
        "from PJ4_model import *\n",
        "import os\n",
        "import logging"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVvpVDGlpRPS"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amLOwMMFpRPS"
      },
      "source": [
        "# Define a train function.\n",
        "def train(epoch, model, criterion, optimizer, dataloader, device):\n",
        "    model.train()\n",
        "    bar = tqdm(enumerate(dataloader))\n",
        "    \n",
        "    samples = 0\n",
        "    total_loss, total_acc =0, 0\n",
        "    for batch_idx, (data,label) in bar:\n",
        "        \n",
        "        data = data.type(torch.FloatTensor).to(device)\n",
        "        label = label.to(device)\n",
        "        \n",
        "        # Pass the input data through the defined network architecture.\n",
        "        pred = model(data, extract=True)\n",
        "\n",
        "        # Compute a loss function.\n",
        "        loss = criterion(pred, label) #크로스엔트로피로 로스를 측정\n",
        "        total_loss += loss.item() * len(label) #로스 더하고\n",
        "        \n",
        "        acc = torch.sum(torch.eq(torch.argmax(pred,-1), label)).item()\n",
        "        samples += len(label)\n",
        "        total_acc += acc\n",
        "        \n",
        "        # Perform backpropagation to update network parameters.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward() #백프로파게이션\n",
        "        optimizer.step() #웨이트 업데이트\n",
        "        \n",
        "        bar.set_description('Epoch:{:3d} [{}/{} {:.2f}%] CE Loss: {:.3f} ACC: {:.2f}'.format(\n",
        "                            epoch, batch_idx, len(dataloader), 100.*(batch_idx/len(dataloader)),\n",
        "                            total_loss / samples, (total_acc / samples)*100.))   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb1EvfBBpRPU"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeBOpQlspRPV"
      },
      "source": [
        "# Define a test function.\n",
        "def test(model, criterion, dataloader, device):\n",
        "    model.eval()\n",
        "    bar = tqdm(dataloader)\n",
        "    \n",
        "    samples = 0\n",
        "    total_loss, total_acc =0, 0\n",
        "    for batch_idx, (data,label) in enumerate(bar):        \n",
        "        \n",
        "        data = data.type(torch.FloatTensor).to(device)\n",
        "        label = label.to(device)        \n",
        "        \n",
        "        # Pass the input data through the defined network architecture.\n",
        "        pred = model(data, extract=True)       \n",
        "        \n",
        "        # Compute a loss function.\n",
        "        loss = criterion(pred, label)\n",
        "        total_loss += loss.item()*len(label)\n",
        "        \n",
        "        samples += len(label)\n",
        "        acc = torch.sum(torch.eq(torch.argmax(pred,-1),label)).item()\n",
        "        total_acc += acc\n",
        "        \n",
        "#         \"\"\"\n",
        "#         bar.set_description('Epoch:{:3d} [{}/{} {:.2f}%] CE Loss: {:.3f} ACC: {:.2f}'.format(\n",
        "#                                 epoch, batch_idx,len(dataloader), 100.*(batch_idx/len(dataloader)),\n",
        "#                                 total_loss / samples,\n",
        "#                                 (total_acc / samples)*100.))\n",
        "#         \"\"\"\n",
        "    return total_loss/samples, (total_acc/samples)*100."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ_X5We4pRPY"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp4oB4pWpRPY"
      },
      "source": [
        "# Define a collate (stacking) function for the data loader.\n",
        "def collate_fn(samples):\n",
        "    data, label = [], []\n",
        "    min_length = min([len(d[0]) for d in samples])-1\n",
        "\n",
        "    for d, l in samples:\n",
        "        st = np.random.randint(len(d)-min_length)\n",
        "        data.append(torch.tensor(d[st:st+min_length]).unsqueeze(0))\n",
        "        label.append(torch.tensor(l))\n",
        "    data = torch.cat(data, 0)\n",
        "    label = torch.LongTensor(label)\n",
        "\n",
        "    return data, label\n",
        "\n",
        "# Obtain the dataset and dataloader of train and test data.\n",
        "def get_dataloader(train_path, test_path, data_path, feature_type ='mel', n_coeff=64):\n",
        "    train_dataset = SpeakerDataset(train_path, data_path, \n",
        "                                   feature_type=feature_type, n_coeff=n_coeff)\n",
        "    test_dataset = SpeakerDataset(test_path, data_path, \n",
        "                                  feature_type=feature_type, n_coeff=n_coeff)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, \n",
        "                              batch_size=8, \n",
        "                              shuffle=True, \n",
        "                              collate_fn=collate_fn,\n",
        "                              num_workers=0, \n",
        "                              pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, \n",
        "                             batch_size=1, \n",
        "                             shuffle=False, \n",
        "                             collate_fn=collate_fn,\n",
        "                             num_workers=0, \n",
        "                             pin_memory=True)\n",
        "    \n",
        "    return train_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJhwfADOpRPc"
      },
      "source": [
        "## Main Function\n",
        "    Define Settings\n",
        "    Get Dataset\n",
        "    Get model\n",
        "    (Load model)\n",
        "    Train and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir6U4cYvpRPc"
      },
      "source": [
        "def main(model_path=None):\n",
        "    ########################################### Settings ##############################################\n",
        "    # Set the configuration for training.\n",
        "    epochs = 1000            # number of epochs\n",
        "    lr = 0.01                # initial learning rate\n",
        "    n_spk = 29              # number of speakers in the dataset    #스피커가 몇명이냐\n",
        "    log_path = 'dvector.log' # log file   \n",
        "    feature_type = 'mel'    # input feature type  #어떤 피쳐\n",
        "    n_coeff = 13             # feature dimension    #차수\n",
        "    indim = n_coeff*3        # input dimension (MFCC, delta, delta-delta) #시간축 변화량도 쓴다. 그래서 총 39차원\n",
        "    context_len = 10         # number of context window  #몇개의 프레임을 합쳐서 입력을 주겠다. (10개 프레임, 아래 코드 보면 10ms 씩 10개 : 100ms)\n",
        "    outdim = 512             # d-vector output dimension\n",
        "    #config 파일 만들고 읽는게 더 좋다.\n",
        "  \n",
        "    \n",
        "    # Check if we can use a GPU device.\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    # Define the directory path of training and test datasets. \n",
        "    train_path = './wsj_train1.txt' #0~28\n",
        "    test_path = './wsj_test1.txt' #0~28\n",
        "    data_path = '..' #트레이닝한 데이터\n",
        "    \n",
        "    ############################### Dataset and dataloader #############################################\n",
        "    train_loader, test_loader = get_dataloader(train_path, test_path, data_path, \n",
        "                                               feature_type=feature_type, n_coeff=n_coeff)\n",
        "      \n",
        "    #################################      Get model       #############################################\n",
        "\n",
        "    # Define a network model.\n",
        "    model = Dvector(n_spk, indim * context_len, outdim).to(device)\n",
        "    \n",
        "    # Set the optimizer with Adam.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    # Set the training criterion.\n",
        "    softmax_criterion = nn.CrossEntropyLoss(reduction='mean')   #한사람만 나오니까 cross entropy loss\n",
        "\n",
        "    #################################### Load pre-trained model ########################################\n",
        "    start = 0\n",
        "    # Load the pre-trained model., 트레이닝된 모델이 있으면 거기부터 트레이닝을 시작한다.\n",
        "    print('Directory of the pre-trained model: {}'.format(model_path))\n",
        "    if model_path:\n",
        "        check = torch.load(model_path)\n",
        "        start = check['epoch']\n",
        "        model.load_state_dict(check['model'])\n",
        "        optimizer.load_state_dict(check['optimizer'])\n",
        "        print('## Successfully load the model at {} epochs!'.format(start))        \n",
        "                                                            \n",
        "    ####################################      Train and Test     ######################################\n",
        "    prev_loss = 10000\n",
        "    ct_dec = 0\n",
        "\n",
        "    for epoch in range(start, epochs+1):\n",
        "\n",
        "        # Train the network.\n",
        "        train(epoch, model, softmax_criterion, optimizer, train_loader, device)\n",
        "        \n",
        "        # Test the network.\n",
        "        opt_loss, opt_acc = test(model, softmax_criterion, test_loader,device)\n",
        "        \n",
        "        # Print out the results.\n",
        "#        print(\"Epoch: {} Test Loss: {:.3f} Test ACC: {:.2f}\".format(epoch,opt_loss,opt_acc ))\n",
        "        \n",
        "        # Save the optimal model.\n",
        "        if opt_loss < prev_loss:\n",
        "            prev_loss = opt_loss            \n",
        "            torch.save({'epoch': epoch,\n",
        "                        'model': model.state_dict(),\n",
        "                        'optimizer': optimizer.state_dict()},\n",
        "                        './model/model_opt_mel.pth') #토치 모델 저장하는 포맷이다.\n",
        "            ct_edec = 0\n",
        "        else:\n",
        "            ct_dec += 1\n",
        "            \n",
        "            # Decrease the learning rate by 2 when the test loss decreases 3 times in a row.\n",
        "            if ct_dec == 3: #3번 이상 로스가 떨어지지 않으면 러닝 레이트를 반으로 줄인다. (트레이닝 트릭)\n",
        "                optim_state = optimizer.state_dict()\n",
        "                optim_state['param_groups'][0]['lr'] /= 2 #러닝레이트 고정하지 않고, 로스펑션을 가져와서 실행한다.\n",
        "                optimizer.load_state_dict(optim_state)\n",
        "                print('lr is divided by 2.')\n",
        "                ct_dec = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "K471te2epRPe",
        "outputId": "2d22cf30-85df-44ba-d5f2-2a10ad2189ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "main(model_path='./model/model_opt_mel.pth')  #이전에 만든 모델부터 시작해서 계속 트레이닝을 해나간다.\n",
        "#main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Directory of the pre-trained model: ./model/model_opt_mel.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "## Successfully load the model at 0 epochs!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0 [487/488 99.80%] CE Loss: 5.828 ACC: 4.36: : 488it [49:21,  6.07s/it]\n",
            "100%|██████████| 300/300 [03:51<00:00,  1.30it/s]\n",
            "Epoch:  1 [487/488 99.80%] CE Loss: 4.957 ACC: 4.23: : 488it [15:42,  1.93s/it]\n",
            "100%|██████████| 300/300 [01:14<00:00,  4.02it/s]\n",
            "Epoch:  2 [487/488 99.80%] CE Loss: 4.552 ACC: 4.69: : 488it [15:42,  1.93s/it]\n",
            "100%|██████████| 300/300 [01:15<00:00,  3.99it/s]\n",
            "Epoch:  3 [487/488 99.80%] CE Loss: 4.152 ACC: 6.18: : 488it [15:37,  1.92s/it]\n",
            "100%|██████████| 300/300 [01:14<00:00,  4.00it/s]\n",
            "Epoch:  4 [487/488 99.80%] CE Loss: 4.012 ACC: 6.77: : 488it [15:36,  1.92s/it]\n",
            "100%|██████████| 300/300 [01:14<00:00,  4.01it/s]\n",
            "Epoch:  5 [487/488 99.80%] CE Loss: 3.526 ACC: 11.03: : 488it [15:34,  1.92s/it]\n",
            "100%|██████████| 300/300 [01:14<00:00,  4.03it/s]\n",
            "Epoch:  6 [487/488 99.80%] CE Loss: 3.642 ACC: 12.57: : 488it [15:33,  1.91s/it]\n",
            "100%|██████████| 300/300 [01:14<00:00,  4.05it/s]\n",
            "Epoch:  7 [487/488 99.80%] CE Loss: 3.443 ACC: 16.52: : 488it [15:35,  1.92s/it]\n",
            "100%|██████████| 300/300 [01:15<00:00,  4.00it/s]\n",
            "Epoch:  8 [487/488 99.80%] CE Loss: 3.367 ACC: 19.96: : 488it [15:36,  1.92s/it]\n",
            "100%|██████████| 300/300 [01:14<00:00,  4.00it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "lr is divided by 2.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  9 [487/488 99.80%] CE Loss: 1.938 ACC: 40.51: : 488it [15:38,  1.92s/it]\n",
            "100%|██████████| 300/300 [01:15<00:00,  3.98it/s]\n",
            "Epoch: 10 [487/488 99.80%] CE Loss: 2.024 ACC: 41.59: : 488it [15:46,  1.94s/it]\n",
            "100%|██████████| 300/300 [01:16<00:00,  3.94it/s]\n",
            "Epoch: 11 [487/488 99.80%] CE Loss: 2.227 ACC: 36.79: : 488it [15:43,  1.93s/it]\n",
            "100%|██████████| 300/300 [01:15<00:00,  3.99it/s]\n",
            "Epoch: 12 [487/488 99.80%] CE Loss: 2.416 ACC: 36.74: : 488it [15:42,  1.93s/it]\n",
            "100%|██████████| 300/300 [01:15<00:00,  4.00it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "lr is divided by 2.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 13 [487/488 99.80%] CE Loss: 1.391 ACC: 57.72: : 488it [15:46,  1.94s/it]\n",
            "100%|██████████| 300/300 [01:15<00:00,  3.99it/s]\n",
            "Epoch: 14 [487/488 99.80%] CE Loss: 1.465 ACC: 57.05: : 488it [15:46,  1.94s/it]\n",
            "100%|██████████| 300/300 [01:15<00:00,  3.96it/s]\n",
            "Epoch: 15 [487/488 99.80%] CE Loss: 1.563 ACC: 54.54: : 488it [15:47,  1.94s/it]\n",
            "100%|██████████| 300/300 [01:15<00:00,  3.98it/s]\n",
            "Epoch: 16 [487/488 99.80%] CE Loss: 1.740 ACC: 53.82: : 488it [15:44,  1.94s/it]\n",
            "100%|██████████| 300/300 [01:15<00:00,  3.97it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "lr is divided by 2.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 17 [487/488 99.80%] CE Loss: 1.049 ACC: 70.22: : 488it [15:36,  1.92s/it]\n",
            "100%|██████████| 300/300 [01:14<00:00,  4.03it/s]\n",
            "Epoch: 18 [487/488 99.80%] CE Loss: 0.908 ACC: 72.93: : 488it [15:33,  1.91s/it]\n",
            "100%|██████████| 300/300 [01:14<00:00,  4.01it/s]\n",
            "Epoch: 19 [487/488 99.80%] CE Loss: 1.016 ACC: 70.55: : 488it [15:31,  1.91s/it]\n",
            "100%|██████████| 300/300 [01:14<00:00,  4.04it/s]\n",
            "Epoch: 20 [487/488 99.80%] CE Loss: 1.145 ACC: 68.32: : 488it [15:30,  1.91s/it]\n",
            "100%|██████████| 300/300 [01:14<00:00,  4.05it/s]\n",
            "Epoch: 21 [487/488 99.80%] CE Loss: 1.166 ACC: 67.86: : 488it [15:32,  1.91s/it]\n",
            "100%|██████████| 300/300 [01:14<00:00,  4.05it/s]\n",
            "Epoch: 22 [487/488 99.80%] CE Loss: 1.364 ACC: 64.73: : 488it [15:21,  1.89s/it]\n",
            "100%|██████████| 300/300 [01:13<00:00,  4.10it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "lr is divided by 2.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 23 [487/488 99.80%] CE Loss: 0.650 ACC: 81.89: : 488it [15:06,  1.86s/it]\n",
            "100%|██████████| 300/300 [01:12<00:00,  4.12it/s]\n",
            "Epoch: 24 [487/488 99.80%] CE Loss: 0.608 ACC: 83.30: : 488it [15:10,  1.87s/it]\n",
            "100%|██████████| 300/300 [01:11<00:00,  4.18it/s]\n",
            "Epoch: 25 [487/488 99.80%] CE Loss: 0.642 ACC: 82.63: : 488it [15:01,  1.85s/it]\n",
            "100%|██████████| 300/300 [01:12<00:00,  4.17it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "lr is divided by 2.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 26 [487/488 99.80%] CE Loss: 0.418 ACC: 89.23: : 488it [15:01,  1.85s/it]\n",
            "100%|██████████| 300/300 [01:11<00:00,  4.17it/s]\n",
            "Epoch: 27 [487/488 99.80%] CE Loss: 0.430 ACC: 89.05: : 488it [15:01,  1.85s/it]\n",
            "100%|██████████| 300/300 [01:12<00:00,  4.13it/s]\n",
            "Epoch: 28 [487/488 99.80%] CE Loss: 0.449 ACC: 88.69: : 488it [15:01,  1.85s/it]\n",
            "100%|██████████| 300/300 [01:11<00:00,  4.19it/s]\n",
            "Epoch: 29 [487/488 99.80%] CE Loss: 0.428 ACC: 88.79: : 488it [14:58,  1.84s/it]\n",
            "100%|██████████| 300/300 [01:12<00:00,  4.16it/s]\n",
            "Epoch: 30 [71/488 14.55%] CE Loss: 0.419 ACC: 88.19: : 72it [02:12,  1.83s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zelC5CYLpRPi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}